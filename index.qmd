---
title: "Principle Component Analysis"
author: "Jordan Iserman, Jeshurun Moses, & Harsha Pola"
date: '`r Sys.Date()`'
format:
   html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

```{r,echo=FALSE}
library(readxl)
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(gtable)
# Read the excel file into a data frame
player.df <- read_excel("C:/Users/theji/Documents/Advanced Statistical Modelling/nba.xlsx")
# Check the structure of the data frame
#str(player.df)
```

# Introduction

Principal Component Analysis (PCA) is a statistical technique that can be used to reduce the dimensionality of complex data sets and highlight key variables. PCA was used for analysis of player techniques in different sports, such as football, basketball, rugby, dance, diving, gymnastics, skiing and more to figure out what those variables are that could have an important influence on the development of training programs, performance analysis or identification of talents within these sports [@bro]. PCA was used to identify the most relevant variables [@pino] in soccer, basketball, and rugby. The systematic review encompassed 34 studies and employed PCA for robust data analysis. In the realm of sports science, this has profound implications: Performance Enhancement, Tailored Training Programs, Performance Analysis, and Talent Identification. This method can assist in pinpointing the pivotal factors contributing to athletes' performance, design personalized training regimes, cater to individual athletes' strengths and weaknesses, facilitate in-depth analysis of performance data, offer insights into areas for improvement, and help in identifying and nurturing talented athletes, crucial for sports development.

A combination approach involving PCA, Random Forest and Genetic Algorithm to classify sport actions based on 3D motion data [@weiwei]. PCA has been used in order to reduce dimensionality of the data, a hybrid classification scheme combining PCA and RF is proposed. The report presents an innovative approach to the classification of sporting actions which relies on 3D motion data derived from advanced techniques, such as PCA, RF and GA, with a view to achieving superior accuracy.

Finally, PCA is a key Statistical Method for Sports Science which may contribute to reducing the dimensionality of complex data sets and highlighting critical variables. It will have a profound impact on the enhancement of performance, tailored training programs, analysis of individual performances and identification of talent. Innovation approaches to the use of PCAs in sports science were demonstrated by [@pino] and [@weiwei].

## Literature Review



When performing principal component analysis, it creates linear combinations of variables. In this article we see that 44 red wine samples were collected all originating from the same grape variety, Cabernet Sauvignon [@bro]. This study uses Foss Wine scan instrument to conduct measurements on 14 characteristics parameters of these wines like ethanol content, pH content etc. This showcases how Principal Component Analysis can be an important tool for extracting meaningful patterns with insights . Researchers apply principal component analysis and linear regression to study quality of drinking water to find pollution sources. Five principal components were used, and 83.1% of the variance is explained by these five components. It was found that the primary forms of pollution was waste from cities [@mustapha]. Researchers believe that the Covid-19 pandemic may influence investors' decisions to invest more or less money in the stock market. In one study, stock market data was analyzed along with the psychology of the investors [@naseem]. The negative emotions experienced by investors made them more likely to avoid investing in the stock market. Investors cared more about their health and well-being than their wealth. Principal component analysis is explained using an example of food choices in different countries. The data set used has 17 dimensions, but these dimensions are reduced to 2 principal components that account for 97% of the variation. This article shows an effective use of principal component analysis, by obtaining an accurate model that is much simpler than the original data set [@richardson]. Principal component analysis was used to analyze dietary habits of Polish esports players. Subjects were given a questionnaire about their dietary habits. The questionnaire consisted of 145 different food items, that were reduced to 37 groups of food. The subjects were also asked about the frequency at which they consume these food items. Other dietary habits were analyzed, such as meal frequency and hydration [@szot].

Researchers have identified Bangladesh as an area that will be significantly vulnerable to climate change. This study was done to reduce the identified vulnerabilities into categories of vulnerability. Principle component analysis was used to reduce a list of 31 indicators to 7, which were Demographic Vulnerability, Economic Vulnerability, Agricultural Vulnerability, Water Vulnerability, Health Vulnerability, Climate Vulnerability, and Infrastructural Vulnerability [@uddin]. In this article we see that the author has taken an approach to combine Detrending Moving-Average Cross Validation Analysis(DMCA) and Principal Component Analysis (PCA), comparing its performance to traditional PCA methods to  determine the primary components of air pollutants [@dong]. The application of DMCA based PCA shows a promising advancement in addressing non–stationary signals in air pollutant data. This method-enhanced reliability, resistance, and the ability to identify key pollutant makes it a  valuable tool in real life.
We see that the components vary seasonally, with winter and autumn having stable pattern.

PCA  has been used in various sports, including dance, diving, gymnastics, skiing, and other sports to analyze the technique of a player. In sports, technique is often a critical factor for success and a defining characteristic of top athletes. PCA is an useful method in sports because it is essential to know the moments, strengths, and weaknesses of our own performance to improve. Identifying potential changes in an athlete’s technique is a major challenge [@gloersen]. In this article we see the importance of assessing performance using notational analysis to identify critical patterns and events that can impact the outcomes of a team. Notational analysis in sports has become a critical tool for identifying key patterns and events that can lead to a successful outcome. Various statistical techniques are used to analyze the data and PCA is one of the most important technique to visualize tactical and technical responses [@rojas]. One article explores the social effects of intelligent sports systems using PCA and fuzzy control technology [@chen]. It investigates how these systems respond to users, focusing on interface design and division of data matrices. The goal is to understand their societal implications . The prominence of perceived usefulness as the most pivotal factor emphasizes its paramount importance in engaging users. The article emphasizes the necessity of understanding the broader societal impact of these systems given the ever-evolving technological landscape. It acknowledges potential research constraints, such as sample size and reliance on self-reported data, which warrant additional investigation.

Another article classifies sports actions using 3D motion data [@weiwei]. The article proposes a combination approach that involves Principal Component Analysis, Random Forest, and Genetic Algorithm. This article uses 3D motion data consisting of 114 dimensional indicators as input features for classification. A hybrid classification model combining PCA and RF is proposed. PCA is noted for its advantage in dimensionality reduction, but doesn't lose the essential features of the original data. Another paper conducts a systematic review focusing on the application of Principal Component Analysis (PCA) in sports science, with a specific emphasis on soccer, basketball, and rugby [@pino]. The primary objective is to identify the most pertinent variables for tasks, such as training program development, performance analysis, and talent identification. The importance of this paper lies in the fact that PCA, a robust statistical technique, can effectively reduce complex data sets' dimensionality and highlight key variables. The methodology adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. It encompasses a comprehensive search of electronic databases like PubMed, Scopus, and Web of Science.

The article titled "Multivariate Exploratory Comparative Analysis of LaLiga Teams" employs Principal Component Analysis (PCA) to evaluate the performance of LaLiga teams with the aim of identifying key performance indicators (KPIs) that distinguish successful teams from less successful ones [@stone]. This study analyzed data from three seasons (2015/16, 2016/17, and 2017/18), encompassing 57 performance indicators, categorized into goal scoring, offensive, and defensive metrics. The findings indicate that successful teams excel in successful passes, dynamic offensive transitions, and effective shots and crosses, while less successful teams tend to prioritize defensive actions, struggle with goal-scoring, and maintain longer ball possession times in the final third of the field. PCA's application offers a robust, multivariate approach to comprehending performance differences among teams. The study underscores the importance of KPIs in sports analysis and management for predicting and enhancing performance. These insights can be applied across various sports, informing the development of performance analysis tools and m ethodologies. Ultimately, this research illuminates essential performance indicators in football and their broader relevance in understanding sports success.

Another research article delves into the complexities of monitoring athlete performance in team sports, specifically focusing on the challenges faced in NCAA Division-I Men’s Basketball competitions [@casal]. The central issue addressed is the often intricate and poorly understood metrics generated by emerging technologies in this context. To simplify these datasets and derive meaningful insights, the authors advocate for the application of Principal Component Analysis (PCA). The article meticulously details the methodology employed and shares the outcomes of the PCA, which was conducted on external load data collected during DI basketball competitions. The primary objective was to streamline this data, enhancing its simplicity. Additionally, the study investigated whether the PCA results exhibited sensitivity to the diverse load demands placed on different positional groups (POS) within the sport. In a descriptive format, the article unveils the PCA findings, spotlighting variations in external load requirements across various positional groups. It also outlines the development of daily individual player preparedness reports, highlighting their distribution to coaching staff through collaboration with performance and sports medicine personnel.

Another method is called adaptive dimensionality reduction (ADR), which is a two-step process [@migenda]. In the first step, ADR uses a neural network to learn the principal components of the data. In the second step, ADR uses a stopping rule to determine the number of principal components to keep. The stopping rule in ADR is based on the idea that if the principal components are not changing significantly, then ADR stops learning new principal components. The authors evaluated ADR on a variety of data sets and found that it was able to achieve better performance than traditional PCA methods.


PCA has found wide-ranging applications, including data visualization, clustering, and classification. Recent advancements in PCA encompass its application to large-scale datasets (big data), integration with other machine learning techniques, and the development of more robust PCA approaches capable of handling outliers with greater resilience [@jolliffe]. This method hinges on the concept of segregating the illumination and reflection components within the image, effectively addressing the challenges posed by low-light conditions. In another study, the authors conducted thorough evaluations of this approach across a diverse range of low-light images, and their findings indicate a noteworthy enhancement in image quality, demonstrating the method's effectiveness [@singh].


# Methods

The goal of this project is to demonstrate an effective usage of principal component analysis on a large dataset. In this section, principal component analysis is explained, as well as the covariance matrix.

## Covariance Matrix

The purpose of calculating the covariance matrix in data analysis is to understand the relationships between variables in a dataset [@jaadi]. This matrix is particularly valuable in identifying correlations and redundancies among variables. The primary goal is to analyze how the variables in a dataset vary in relation to each other. This analysis helps uncover any existing relationships or dependencies between variables. The covariance matrix is a square matrix with dimensions p × p, where p is the number of variables (dimensions) in the dataset. For a 3-dimensional dataset with variables x, y, and z, the covariance matrix is a 3x3 matrix. The covariance matrix is symmetric, meaning that covariances between variables are symmetric with respect to the main diagonal. In other words, the covariance of x with y is the same as the covariance of y with x. The entries (elements) of the covariance matrix represent covariances between pairs of initial variables. For a 3x3 matrix, the entries are:

\begin{bmatrix}
Cov(x,x) & Cov(x,y) & Cov(x,z)\\
Cov(y,x) & Cov(y,y) & Cov(y,z)\\
Cov(z,x) & Cov(z,y) & Cov(z,z)
\end{bmatrix}

The formula to calculate $Cov(X,Y)$ for any random variables, $X$ and $Y$ is as follows [@chiou]:

$$Cov(X,Y) = E[(X − E[X])(Y − E[Y])]$$

A positive covariance indicates a positive linear relationship between two variables, while a negative covariance suggests a negative linear relationship. A covariance of zero implies no linear relationship. When variables have high positive covariances, it indicates redundancy or similarity in the information they carry. In such cases, one of these variables may be redundant for modeling purposes. While the covariance matrix is a valuable tool for understanding linear relationships between variables, it does not capture the full complexity of data relationships. To explore nonlinear or more subtle associations, additional statistical methods like correlation analysis or principal component analysis (PCA) may be used.

## Principal Component Analysis

Principal component analysis is a method of analyzing a data set that can reduce a large number of variables. Principal component analysis takes two or more variables and combines them into one principal component. A principal component is a linear combination of these variables as a unit vector. The weights of each variables are considered, and the linear combination accounts for the weight of each variable [@bro]. The formula to find these linear combinations is as follows:


$$\underset{||w||=1}{argmax}(t^{T}t) = \underset{||w||=1}{argmax}(w^{T}X^{T}Xw)$$


- *X* is a matrix of the data to be reduced with *i* rows and *j* columns
- *w* is a vector of length *j* with the data of a given column
- *t* is a linear combination of the variables from matrix *X*
- *argmax* returns the largest element in a row or column

To determine the amount of variation in the data, the columns of X may be projected on t by using the formula:

$$X = tp^{T} + E$$

- *p* is the vector of coefficients of regression
- *E* is the matrix of residuals.

The percentage of explained variation is given by the formula:

$$\frac{||X||^{2} - ||E||^{2}}{||X||^{2}}* 100\%$$

If model does not explain a sufficient amount of the variation in the data set, additional components can be added. If there are multiple components, the model is as follows:

$$X = t_{1}p_{1}^{T} + ... + t_{n}p_{n}^{T} + E$$



# Analysis and Results

```{r, echo=FALSE}
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)

nba = player.df[,4:ncol(player.df)]
nba = as.data.frame(sapply(nba, as.numeric ))
names = player.df[,2]
```

```{r, echo=FALSE}
library(imputeTS)

# Impute NAs using the mean
nba_data <- na_mean(nba)

library(ggcorrplot)

# Calculate the correlation matrix
library(psych)

# Select the relevant features for PCA
features <- nba_data[, c(
  "AGE", "GP", "MPG", "MINpct", "USGpct", "TOpct",
  "FTA", "Ftpct", "TwoPA", "pct2P", "ThreePA", "pct3P",
  "eFGpct", "Tspct", "PPG", "RPG", "TRBpct", "APG", "ASTpct",
  "SPG", "BPG", "TOPG", "VI", "ORTG", "DRTG"
)]

# Standardize the features (mean=0, sd=1)
standardized_features <- scale(features)

# Calculate the correlation matrix
correlation_matrix <- cor(standardized_features)
```

```{r, include=FALSE, echo=FALSE}
library(corrplot)

# Create a heatmap of the correlation matrix
corrplot(correlation_matrix, method = "color", type = "upper", tl.cex = 0.7)

#dendrogram that illustrates hierarchical clustering of variables based on their correlations. This can help identify groups of variables that are more strongly related to each other.
library(pvclust)
result <- pvclust(correlation_matrix)
```

```{r,echo=FALSE}
plot(result)
```

This clustering dendrogram shows how similar the variables are. On the right side, it shows that 2-point attempts and free-throw attempts are highly correlated variables, while other variables do not affect these as much. 

```{r,echo=FALSE}
library(lares)
Sys.unsetenv("LARES_FONT")
corr_cross(nba,
           max_pvalue = 0.05, # have only significant correlations (at the 5% level)
           top = 10 # display the first 10 pairs of variables (by correlation coefficient)
)
```

The plot above shows the 10 highest correlations between variables. A correlation of 1 between 2 variables shows that they provide the same information. This makes sense, as both variables are based on how much time a player spends on the court during a game. Some other variables have correlations of lower than one, such as effective field goal percentage and total shooting percentage.

```{r,echo=FALSE}
pca = prcomp(nba_data, scale=T)

#summary(pca)

library(factoextra)
p1_ess <- fviz_eig(pca, addlabels = TRUE, choice="eigenvalue",main ="Eigenvalues of 
                   the explained inertia")
p2_ess <- fviz_eig(pca, addlabels = TRUE, main="Proportion of variances")
grid.arrange(p1_ess, p2_ess, nrow=1,name = "arrange")
```

These charts show how much of the variation is explained by each principal component. The first ten principal components explain 86% of the variation. This means that the 28 dimensions of this data set can be reduced to 10, and only 14% of the variation will be unexplained.

## Data and Visualization

The data used for this analysis was retrieved from the NBA's official website. The data includes statistics about player performance, such as offensive rating and defensive rating, points per game, and free throw percentage. We see the first three variables, `NAME`, `TEAM`, and `POS`, are strings, and the rest of the variables are numerical. Below is the description of the variables.

```{r,echo=FALSE}
library(readxl)
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(gtable)
# Read the excel file into a data frame
player.df <- read_excel("C:/Users/theji/Documents/Advanced Statistical Modelling/nba.xlsx")
# Check the structure of the data frame
#str(player.df)
```

| Variables | Description |
|--|-------------|
`NAME`   |  The full name of the basketball player
`TEAM`    |            The team that the player is a part of
`POS`      |              The player's position on the basketball court (e.g., point guard, shooting guard,           small forward, power forward, center)
`AGE`       | The age of the player
`GP`        | The number of games played by the player
`MPG`      | The average number of minutes the player spends on the court per game
`MINpct`  | The percentage of total team minutes used by the player while they were on the floor.
`USGpct`  |              An estimate of the percentage of team plays used by the player while they were on the floor. It measures how involved the player is in the team's offense.
`TOpct`  |                 The player's turnover rate, a metric that estimates the number of turnovers a player commits per 100 possessions.
`FTA` |                    The number of free throws attempted by the player.
`FTpct` |                   The percentage of free throws made by the player.
`att2P` |                    The number of two-point shots attempted by the player.
`pct2P` |                   The percentage of two-point shots made by the player.
`att3P` |                    The number of three-point shots attempted by the player.
`pct3p` |                   The percentage of three-point shots made by the player.
`eFGpct` |                 A measure that takes into account the value of three-point shots in scoring efficiency.
`TSpct` |                    A measure of overall shooting efficiency that includes field goals, three-pointers, and free throws.
`PPG` |                   The average number of points scored by the player in each game.
`RPG` |                   The average number of rebounds grabbed by the player in each game.
`TRBpct` |                 An estimated percentage of available rebounds grabbed by the player while they are on the court.
`APG` |                    The average number of assists the player records in each game.
`ASTpct` |                  An estimated percentage of teammate field goals a player assisted while they are on the court.
`SPG` |                     The average number of steals the player records in each game.
`BPG` |                    The average number of blocks the player records in each game.
`TOPG` |                  The average number of turnovers committed by the player in each game.
`VI` |                       The player's versatility index, a metric that measures a player's ability to produce in points, assists, and rebounds.
`ORTG` |                   The number of points produced by the player per 100 total individual possessions.
`DRTG` |                    An estimate of how many points the player allowed per 100 possessions they individually faced while on the court. This reflects the player's defensive performance.


```{r, include=FALSE, echo=FALSE}
#summary(player.df)

#is.na(player.df)
#which(colSums(is.na(player.df))>0)
#colSums(is.na(player.df))

#kable(player.df %>%
#        select(everything()) %>%  
#        summarise_all(funs(sum(is.na(.)))))%>%
#  kable_styling(latex_options = c("striped", "scale_down","HOLD_position"))
```

Histograms to show the spread of the variables are shown below.

```{r,echo=FALSE}
# Identify numeric columns for histogram plotting
numeric_columns <- sapply(player.df, is.numeric)

# Get the column names of the numeric columns
numeric_column_names <- colnames(player.df)[numeric_columns]

# Create histograms for numeric columns
histograms <- lapply(numeric_column_names, function(col_name) {
  ggplot(player.df, aes_string(x = col_name)) +
    geom_histogram(binwidth = 5, fill = "skyblue", color = "black")
})

# Arrange the histograms in a grid
grid.arrange(grobs = histograms, ncol = 5)
```

Boxplots with outliers included are shown below.

```{r,echo=FALSE}
# Create boxplots for numeric columns
boxplots <- lapply(numeric_column_names, function(col_name) {
  ggplot(player.df, aes_string(x = col_name)) +
    geom_boxplot(fill = "skyblue", color = "black")
})

# Arrange the boxplots in a grid
grid.arrange(grobs = boxplots, ncol = 5)
```

The histograms shown have a variety of shapes. Some appear to be distributed normally, while others are skewed right. This may be because there are many more average players than there are star-players that have a dominating presence on the court. The boxplots show outliers primarily to the right for most stats to further support the claim that some players perform significantly better than the average players.

```{r,echo=FALSE}
library(RColorBrewer)

fviz_pca_var(pca, col.var = "cos2",
             gradient.cols = brewer.pal(3, "Set1"),
             repel = TRUE,
             title = "Cos 2 of the variables NBA Stats - PCA")
```

In the factorial map above, we observe the relationships between statistical variables of NBA players. We can see that the variables SPG  GP  Ftpct   VI  pct2P  Tspct  FGpcl  are Negatively correlated with the first dimension, while the variables DGRT USGpct  TOPG  APG  MPG  FTA  ThreePA  BPG and TwoBa and positively correlated with the second dimension. Both the dimensions represented on the graph shows 33.4% and 11.5% of the variance of the data respectively. Variables which are positively correlated with high cos2  indicate  a good representation of these axes. Variables which are negatively correlated with low cos2 indicate that the principal axes do not perfectly represent represent the variable. 


```{r,echo=FALSE}
# Contributions of variables to PC1
c1 <- fviz_contrib(pca, "var", axes=1)
# Contributions of variables to PC2
c2 <- fviz_contrib(pca, "var", axes=2)
# Contributions of variables to PC3
c3 <- fviz_contrib(pca, "var", axes=3)
# Contributions of variables to PC4
c4 <- fviz_contrib(pca, "var", axes=4)
# Combine the contributions of the variables to PC1, PC2, PC3, and PC4.
grid.arrange(c1, c2, c3, c4, top='Contribution of NBA stats variables to the principal components')
```

These graphs show the amount that each variable contributes to each dimension.

```{r,echo=FALSE}
barplot(pca$rotation[,1], las=2, col="darkblue", main = "1st component (Dim-1)")
```

In the first dimension the main contributions are from MPG, MINpct, and PPG. TRBpct, TOpct and ORTG are the least contributions.

```{r,echo=FALSE}
barplot(pca$rotation[,2], las=2, col="darkblue", main = "2nd component (Dim-2)")
```
In the second dimension the main contributions are from eFGpvt, Tspct, pct2P and the least dimension is BPG, GP.

```{r,echo=FALSE}
barplot(pca$rotation[,3], las=2, col="darkblue", main = "3rd component (Dim-3)")
```

In the third dimension the main contributions are from TRBpct and USGpct. The least dimension is TwoPA, FTA, PPG and APG.

```{r,echo=FALSE}
barplot(pca$rotation[,4], las=2, col="darkblue", main = "4th component (Dim-4)")
```

In the fourth dimension the main contributions are from DRTG, ASTpct and GP. Tspct, eFGpct are the least contribution.

```{r, echo=FALSE}
#calculateScore = function(data) {
#  return(sum((pca$rotation[, 1]*data)^2))
#}
#player.df$NAME[sort.int(apply(nba, 1, calculateScore), decreasing = T, 
#                                           index.return = T)$ix[1:10]]
```

```{r,echo=FALSE}
#calculateScore = function(data) {
#  return(sum((pca$rotation[, 2]*data)^2))
#}
#nba_data$GP[sort.int(apply(nba, 1, calculateScore), decreasing = T, index.return = T)$ix[1:10]]
```

```{r,echo=FALSE}
fviz_pca_biplot(pca, col.var = "contrib", labelsize = 3, repel = TRUE, geom = "point", point.size = 2, point.color = "blue", title = "PCA Biplot Individuals and variables")
```

Biplot is an overlay of a score plot. This is a common plot in Principal Component Analysis. A biplot allows information on both samples and variables of a data matrix to be displayed graphically. We see the observations and the variables in the same plane usint the first two components.

```{r,echo=FALSE}
fviz_pca_biplot(pca, col.var = "contrib", labelsize = 3, repel = TRUE, title = "PCA Biplot Individuals and variables", xlab = "PC1", ylab = "PC2")
```

## Conclusion

This project shows an application of principal component analysis on an NBA data set. The model shows the relevance of each variable on the overall performance of players. The cosine values of each unit vector shows that many stats are closely related to other stats. For example, 2-point attempts and free-throw attempts are closely related. This may be because players shooting close to the rim are more likely to be fouled and receive a free-throw attempt. The four principal components used include combinations of the variables, and the amount that each variable contributes is shown above in the graph. These four components are constructed by creating linear combinations of the variables as unit vectors. This project shows how a data set with 28 variables can be reduced to a model that consists of four principal components. While principal component analysis may make a model that explains slightly less variation, it has simplicity that can be helpful for understanding a complex data set. 

# References