---
title: "An Analysis of NBA Player Performance Using Principal Component Analysis"
author: "Jordan Iserman, Jeshurun Moses, & Harsha Pola"
date: '`r Sys.Date()`'
format:
   html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib
always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

```{r,echo=FALSE}
library(readxl)
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(gtable)
# Read the excel file into a data frame
player.df <- read.csv("/Users/theji/Documents/Advanced Statistical Modelling/nba.csv", header = TRUE)
# Check the structure of the data frame
#str(player.df)
```

# Introduction

Technology has bought a drastic change in the sports industry. In the 1990's the players performance was calculated on paper, but in this modern generation, everything is done by software. Principal Component Analysis (PCA) is one of the main methods used world wide to find player performances, combining various factors from the game. 
Principal Component Analysis (PCA) is a statistical technique that can be used to reduce the dimensionality of complex data sets and highlight key variables. PCA is used to analyze player performance in different sports, such as soccer, basketball, dance, diving, gymnastics, skiing and more to figure out what those variables are that could have an important influence on the development of training programs, performance analysis, or identification of talents within these sports [@rojas].
In this project, we use PCA to analyze NBA player data with the help of R and Quarto to evaluate player performance considering different aspects of the game. We use different visualization techniques to show the team's performance and an  individual player performance.

Principal component analysis is a method of analyzing a data set that can reduce many variables. Principal component analysis takes two or more variables and combines them into one principal component. A principal component is a linear combination of these variables as a unit vector. The weights of each variable are considered, and the linear combination accounts for the weight of each variable [@bro]. The variables are weighted by using eigenvectors.
The total amount of variance that can be explained by a given principal component is called an Eigenvalue. They can be either positive or negative. If the Covariance Matrix values are greater than zero, then it’s a good sign. Eigenvectors measure how each variable is associated with one another using a Covariance Matrix. These vectors are used to understand the direction of the spread of the data.


## Literature Review

Principal Component Analysis (PCA) is a versatile technique rooted in sophisticated mathematical principles. Its prime role is to convert the original set containing possibly co-related variables into a smaller group called the principle component. Traditionally known as a technique applied to multivariate data PCA has now developed into numerous contexts. Being one of the most significant outcomes of practical linear algebra, PCA is usually used as a first phase for analyzing extensive sets. This utility goes beyond conventional data analysis to include problems such as noise suppression, blind source separation, and data compression. However, it simplifies the large data or in essence, PCA is a foundation stone that facilitates exploring and interpreting the same. PCA reduces the complexity of huge datasets using the linear transformation of the vector place. Principal components are derived mathematically from an initial set of data which may consist of many variables. Through this process of dimensional reduction, data can be more intuitively interpreted, often summarized by only a few main factors. Through careful assessment of the compact data, users can easily identify trends, patterns, and outliers that could not have been detected if it had not been for the PCA application [@richardson].

This paper provides an in-depth understanding of PCA use, interpreting, and understanding. Nevertheless, it is employed for traditional chemometric contexts but its findings may apply in other unrelated science fields. It entails generating a linear combination of variables for PCA.  Another journal demonstrates one case study for 44 red wines taken from a single type of grape sauvignon blanc [@bro]. This study conducted a delicate inspection involving parameter tests including alcohol content and acidity using the foss wine scan machine. As illustrative research, this study provides evidence that Principal Component Analysis is one of the powerful tools for the identification of meaningful patterns and in-depth comprehension of complex datasets.
The advent of 3D motion capture technology has generated a great number of three-dimensional motion samples, which has increased interest in sports data classification and recognition. In this study a hybrid approach that employs PCA, RF, and GA featuring 114-dimensional elements for classifying sports motion classification. The development of the GA-PCA-RF model revealed that it was highly accurate in action prediction of the three types. The use of PCA for dimensionality reduction improves classification accuracy besides that of GA-based parameter optimization. This is a fresh and useful classification of sports activities among humans [@weiwei]. 

Another method is called adaptive dimensionality reduction (ADR), which is a two-step process [@migenda]. In the first step, ADR uses a neural network to learn the principal components of the data. In the second step, ADR uses a stopping rule to determine the number of principal components to keep. The stopping rule in ADR is based on the idea that if the principal components are not changing significantly, then ADR stops learning new principal components. The authors evaluated ADR on a variety of data sets and found that it was able to achieve better performance than traditional PCA methods.

PCA has found wide-ranging applications, including data visualization, clustering, and classification. Recent advancements in PCA encompass its application to large-scale datasets (big data), integration with other machine learning techniques, and the development of more robust PCA approaches capable of handling outliers with greater resilience [@jolliffe]. This method hinges on the concept of segregating the illumination and reflection components within the image, effectively addressing the challenges posed by low-light conditions. In another study, the authors conducted thorough evaluations of this approach across a diverse range of low-light images, and their findings indicate a noteworthy enhancement in image quality, demonstrating the method’s effectiveness [@singh].

Another research article delves into the complexities of monitoring athlete performance in team sports, specifically focusing on the challenges faced in NCAA Division-I Men’s Basketball competitions [@casal]. The central issue addressed is the often intricate and poorly understood metrics generated by emerging technologies in this context. To simplify these datasets and derive meaningful insights, the authors advocate for the application of Principal Component Analysis (PCA). The article meticulously details the methodology employed and shares the outcomes of the PCA, which was conducted on external load data collected during DI basketball competitions.



# Methods

The goal of this project is to demonstrate an effective usage of principal component analysis on a large dataset. In this section, principal component analysis is explained, as well as the covariance matrix.

## Covariance Matrix

The purpose of calculating the covariance matrix in data analysis is to understand the relationships between variables in a dataset [@jaadi]. This matrix is particularly valuable in identifying correlations and redundancies among variables. The primary goal is to analyze how the variables in a dataset vary in relation to each other. This analysis helps uncover any existing relationships or dependencies between variables. The covariance matrix is a square matrix with dimensions p × p, where p is the number of variables (dimensions) in the dataset. For a 3-dimensional dataset with variables x, y, and z, the covariance matrix is a 3x3 matrix. The covariance matrix is symmetric, meaning that covariances between variables are symmetric with respect to the main diagonal. In other words, the covariance of x with y is the same as the covariance of y with x. The entries (elements) of the covariance matrix represent covariances between pairs of initial variables. For a 3x3 matrix, the entries are:

\begin{bmatrix}
Cov(x,x) & Cov(x,y) & Cov(x,z)\\
Cov(y,x) & Cov(y,y) & Cov(y,z)\\
Cov(z,x) & Cov(z,y) & Cov(z,z)
\end{bmatrix}

The formula to calculate $Cov(X,Y)$ for any random variables, $X$ and $Y$ is as follows [@chiou]:

$$Cov(X,Y) = E[(X − E[X])(Y − E[Y])]$$

A positive covariance indicates a positive linear relationship between two variables, while a negative covariance suggests a negative linear relationship. A covariance of zero implies no linear relationship. When variables have high positive covariances, it indicates redundancy or similarity in the information they carry. In such cases, one of these variables may be redundant for modeling purposes. While the covariance matrix is a valuable tool for understanding linear relationships between variables, it does not capture the full complexity of data relationships. To explore nonlinear or more subtle associations, additional statistical methods like correlation analysis or principal component analysis (PCA) may be used.

## Principal Component Analysis

### Linear Combinations

Principal Component Analysis reduces data sets by creating linear combinations of variables. A linear combination of variables is given as follows:

$$t = c_{1}X_{1} + ... + c_{n}X_{n}$$
where $X_{n}$ are variables in the data set, and *t* is a linear combination of the variables used to calculate it. *t* may also be written as a vector:

\begin{bmatrix}
X_{1}\\
:\\
X_{n}
\end{bmatrix}

*t* may also be written as a product of the matrix, *X*, and the vector, *c*:

$$t = Xc$$

The goal for principal component analysis is to have an adequate amount of variation in the principal components that are used to reduce the data set. If there is enough variation explained in *t*, then multiple variables can be replaced with one principal component. Ideally, the variance of *t* , var(*t*), is large. However, this value must be normalized, which is given by the following formula.

$$\underset{||c||=1}{argmax}(var(t)) $$ 

This problem can be understood as finding a *c*, or combination of variables, of length one that will maximize the variance of *t*. The *argmax* function returns the position of the maximum value [@bro]. Since $t = Xw$, the following formula shows how the *argmax* function would look if *t* was replaced with *Xc*


$$\underset{||c||=1}{argmax}(t^{T}t) = \underset{||c||=1}{argmax}(c^{T}X^{T}Xc)$$


- *X* is a matrix of the data to be reduced with *i* rows and *j* columns
- *c* is a vector of length *j* with the data of a given column
- *t* is a linear combination of the variables from matrix *X*
- *argmax* returns the largest element in a row or column

### Variation

To determine the amount of variation in the data, the columns of X may be projected on t by using the formula:

$$X = tp^{T} + E$$

- *p* is the vector of coefficients of regression
- *E* is the matrix of residuals.

The percentage of explained variation is given by the formula:

$$\frac{||X||^{2} - ||E||^{2}}{||X||^{2}}* 100\%$$

If model does not explain a sufficient amount of the variation in the data set, additional components can be added. If there are multiple components, the model is as follows:

$$X = t_{1}p_{1}^{T} + ... + t_{n}p_{n}^{T} + E$$

# Data and Visualization

The data used for this analysis was retrieved from the NBA's official website. The data includes statistics about player performance, such as offensive rating and defensive rating, points per game, and free throw percentage. We see the first three variables, `NAME`, `TEAM`, and `POS`, are strings, and the rest of the variables are numerical. Below is the description of the variables.

```{r,echo=FALSE}
library(readxl)
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(gtable)
library(RColorBrewer)
library(factoextra)
library(imputeTS)
library(psych)
library(ggcorrplot)
library(corrplot)
library(lares)
library(pvclust)


# Read the excel file into a data frame
player.df <- read.csv("/Users/theji/Documents/Advanced Statistical Modelling/nba.csv", header = TRUE)
# Check the structure of the data frame
#str(player.df)
nba = player.df[,4:ncol(player.df)]
nba = as.data.frame(sapply(nba, as.numeric ))
names = player.df[,1]
nba_data <- na_mean(nba)
pca = prcomp(nba_data, scale=T)
features <- nba_data[, c(
  "AGE", "GP", "MPG", "MINpct", "USGpct", "TOpct",
  "FTA", "Ftpct", "TwoPA", "pct2P", "ThreePA", "pct3P",
  "eFGpct", "Tspct", "PPG", "RPG", "TRBpct", "APG", "ASTpct",
  "SPG", "BPG", "TOPG", "VI", "ORTG", "DRTG"
)]

# Standardize the features (mean=0, sd=1)
standardized_features <- scale(features)

# Calculate the correlation matrix
correlation_matrix <- cor(standardized_features)
```

| Variables | Description |
|--|-------------|
`NAME`   |  The full name of the basketball player
`TEAM`    |            The team that the player is a part of
`POS`      |              The player's position on the basketball court (e.g., point guard, shooting guard,           small forward, power forward, center)
`AGE`       | The age of the player
`GP`        | The number of games played by the player
`MPG`      | The average number of minutes the player spends on the court per game
`MINpct`  | The percentage of total team minutes used by the player while they were on the floor.
`USGpct`  |              An estimate of the percentage of team plays used by the player while they were on the floor. It measures how involved the player is in the team's offense.
`TOpct`  |                 The player's turnover rate, a metric that estimates the number of turnovers a player commits per 100 possessions.
`FTA` |                    The number of free throws attempted by the player.
`FTpct` |                   The percentage of free throws made by the player.
`att2P` |                    The number of two-point shots attempted by the player.
`pct2P` |                   The percentage of two-point shots made by the player.
`att3P` |                    The number of three-point shots attempted by the player.
`pct3p` |                   The percentage of three-point shots made by the player.
`eFGpct` |                 A measure that takes into account the value of three-point shots in scoring efficiency.
`TSpct` |                    A measure of overall shooting efficiency that includes field goals, three-pointers, and free throws.
`PPG` |                   The average number of points scored by the player in each game.
`RPG` |                   The average number of rebounds grabbed by the player in each game.
`TRBpct` |                 An estimated percentage of available rebounds grabbed by the player while they are on the court.
`APG` |                    The average number of assists the player records in each game.
`ASTpct` |                  An estimated percentage of teammate field goals a player assisted while they are on the court.
`SPG` |                     The average number of steals the player records in each game.
`BPG` |                    The average number of blocks the player records in each game.
`TOPG` |                  The average number of turnovers committed by the player in each game.
`VI` |                       The player's versatility index, a metric that measures a player's ability to produce in points, assists, and rebounds.
`ORTG` |                   The number of points produced by the player per 100 total individual possessions.
`DRTG` |                    An estimate of how many points the player allowed per 100 possessions they individually faced while on the court. This reflects the player's defensive performance.


```{r,include=FALSE, echo=FALSE}
summary(player.df)

is.na(player.df)
which(colSums(is.na(player.df))>0)
colSums(is.na(player.df))

kable(player.df %>%
        select(everything()) %>%  
        summarise_all(funs(sum(is.na(.)))))%>%
  kable_styling(latex_options = c("striped", "scale_down","HOLD_position"))
```

Histograms to show the spread of the variables are shown below.

```{r}
# Identify numeric columns for histogram plotting
numeric_columns <- sapply(player.df, is.numeric)

# Get the column names of the numeric columns
numeric_column_names <- colnames(player.df)[numeric_columns]

# Create histograms for numeric columns
histograms <- lapply(numeric_column_names, function(col_name) {
  ggplot(player.df, aes_string(x = col_name)) +
    geom_histogram(binwidth = 5, fill = "skyblue", color = "black")
})

# Arrange the histograms in a grid
grid.arrange(grobs = histograms, ncol = 5)
```

Boxplots with outliers included are shown below.

```{r}
# Create boxplots for numeric columns
boxplots <- lapply(numeric_column_names, function(col_name) {
  ggplot(player.df, aes_string(x = col_name)) +
    geom_boxplot(fill = "skyblue", color = "black")
})

# Arrange the boxplots in a grid
grid.arrange(grobs = boxplots, ncol = 5)
```

The histograms shown have a variety of shapes. Some appear to be distributed normally, while others are skewed right. This may be because there are many more average players than there are star-players that have a dominating presence on the court. The boxplots show outliers primarily to the right for most stats to further support the claim that some players perform significantly better than the average players.

```{r}
corrplot(correlation_matrix, method = "color", type = "upper", tl.cex = 0.7)
```

The correlation matrix above shows the correlations between every possible combination of variables.  

AGE and MPG: There is a strong positive correlation between AGE and MPG. 

GP and MPG: There is a strong negative correlation between GP and MPG. 

MINpct and MPG: There is a strong negative correlation between MINpct and MPG. 

USGpct and MPG: There is a strong negative correlation between USGpct and MPG.

TOPG and MPG: There is a strong negative correlation between TOPG and MPG. 

FTA and Ftpct: There is a strong positive correlation between FTA and Ftpct. 

Tspct and PPG: There is a strong positive correlation between Tspct and PPG. 

MPG and PPG: There is a strong positive correlation between MPG and PPG. 

```{r}
#=====================================================================================
#ADD THESE WHEREVER YOU THINK THEY FIT. (PPG, 3PS, ASSISSTS)
#TOP 10 PLAYERS BY POINTS PER GAME
# Sorting players by points per game
top_points_players <- player.df[order(player.df$PPG, decreasing = TRUE), ]

# Selecting the top 10 players
top_10_points_players <- top_points_players[1:10, ]

# Creating a bar plot for the top 10 players by points per game
ggplot(top_10_points_players, aes(x = reorder(NAME, -PPG), y = PPG, fill=NAME)) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Top 10 Players by Points Per Game", x = "Player", y = "Points Per Game") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The above graph shows the ten players with the highest points per game.

```{r}

##TOP 10 PLAYERS BY 3-POINTERS PER GAME

# Sorting players by 3-point percentage
top_3p_players <- player.df[order(player.df$pct3P, decreasing = TRUE), ]
# Selecting the top 10 players
top_10_3p_players <- top_3p_players[1:10, ]
# Creating a bar plot for the top 10 players by 3-point percentage
ggplot(top_10_3p_players, aes(x = reorder(NAME, -pct3P), y = pct3P, fill = NAME)) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Top 10 Players by 3-Pointers Percentage", x = "Player", y = "3-Point Percentage") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The above graph shows the ten players with the highest three-point percentage.

```{r}
##TOP 10 PLAYERS BY ASSISTS PER GAME

# Sorting players by assists
top_assists_players <- player.df[order(player.df$APG, decreasing = TRUE), ]

# Selecting the top 10 players
top_10_assists_players <- top_assists_players[1:10, ]

# Creating a bar plot for the top 10 players by assists
ggplot(top_10_assists_players, aes(x = reorder(NAME, -APG), y = APG, fill = NAME)) +
  geom_bar(stat = "identity", color = "black") +
  labs(title = "Top 10 Players by Assists", x = "Player", y = "Assists Per Game") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The above graph shows the ten players with the most assists per game.

# Analysis and Results

```{r}
Sys.unsetenv("LARES_FONT")
corr_cross(nba,
           max_pvalue = 0.05, # have only significant correlations (at the 5% level)
           top = 10 # display the first 10 pairs of variables (by correlation coefficient)
)
```

The plot above shows the 10 highest correlations between variables. A correlation of 1 between 2 variables shows that they provide the same information. This makes sense, as both variables are based on how much time a player spends on the court during a game. Some other variables have correlations of lower than one, such as effective field goal percentage and total shooting percentage.

```{r}
#summary(pca)

p1_ess <- fviz_eig(pca, addlabels = TRUE, choice="eigenvalue",main ="Eigenvalues of 
                   the explained inertia")
p2_ess <- fviz_eig(pca, addlabels = TRUE, main="Proportion of variances")
grid.arrange(p1_ess, p2_ess, nrow=1,name = "arrange")
```

These charts show how much of the variation is explained by each principal component. The first ten principal components explain 86% of the variation. This means that the 28 dimensions of this data set can be reduced to 10, and only 14% of the variation will be unexplained.

```{r}
# Contributions of variables to PC1
c1 <- fviz_contrib(pca, "var", axes=1)
# Contributions of variables to PC2
c2 <- fviz_contrib(pca, "var", axes=2)
# Contributions of variables to PC3
c3 <- fviz_contrib(pca, "var", axes=3)
# Contributions of variables to PC4
c4 <- fviz_contrib(pca, "var", axes=4)
# Combine the contributions of the variables to PC1, PC2, PC3, and PC4.
grid.arrange(c1, c2, c3, c4, top='Contribution of NBA stats variables to the principal components')
```

These graphs show the amount that each variable contributes to each dimension.

```{r, include=FALSE, echo=FALSE}
names = player.df[,1]
grid.arrange(grobs = histograms, ncol = 4)
```



```{r, include=FALSE, echo=FALSE}
calculateScore = function(data) {
  return(sum((pca$rotation[, 1]*data)^2))
}
player.df$NAME[sort.int(apply(nba, 1, calculateScore), decreasing = T, 
                                           index.return = T)$ix[1:10]]

calculateScore = function(data) {
  return(sum((pca$rotation[, 1]*data)^2))
}
player.df$NAME[sort.int(apply(nba, 1, calculateScore), decreasing = T, 
                                           index.return = T)$ix[1:10]]

calculateScore = function(data) {
  return(sum((pca$rotation[, 2]*data)^2))
}
nba_data$GP[sort.int(apply(nba_data, 1, calculateScore), decreasing = T, index.return = T)$ix[1:10]]



head(get_pca_ind(pca)$contrib[,1])

head((pca$x[,1]^2)/(pca$sdev[1]^2))/dim(nba_data)[1]

names[order(get_pca_ind(pca)$contrib[,1],decreasing=T)][1:10]
```



```{r}
names_z1 = names[order(get_pca_ind(pca)$contrib[,1],decreasing=T)]
fviz_contrib(pca, choice = "ind", axes = 1, top=20)+scale_x_discrete(labels=names_z1)
```

This graph shows the players that contributed the most to the first dimension.

```{r}
names_z2 = names[order(get_pca_ind(pca)$contrib[,2],decreasing=T)]
fviz_contrib(pca, choice = "ind", axes = 2, top=20)+scale_x_discrete(labels=names_z2)
```

This graph shows the players that contributed the most to the second dimension.

```{r}
names_z3 = names[order(get_pca_ind(pca)$contrib[,3],decreasing=T)]
fviz_contrib(pca, choice = "ind", axes = 3, top=20)+scale_x_discrete(labels=names_z3)
```

This graph shows the players that contributed the most to the third dimension.

```{r}
names_z4 = names[order(get_pca_ind(pca)$contrib[,4],decreasing=T)]
fviz_contrib(pca, choice = "ind", axes = 4, top=20)+scale_x_discrete(labels=names_z4)
```

This graph shows the players that contributed the most to the fourth dimension.

```{r,echo=FALSE}
b1 <- barplot(pca$rotation[,1], las=2, col="darkblue", main = "1st component (Dim-1)")
```

In the first dimension the main contributions are from MPG, MINpct, and PPG. TRBpct, TOpct and ORTG are the least contributions.

```{r,echo=FALSE}
b2 <- barplot(pca$rotation[,2], las=2, col="darkblue", main = "2nd component (Dim-2)")
```
In the second dimension the main contributions are from eFGpvt, Tspct, pct2P and the least dimension is BPG, GP.

```{r,echo=FALSE}
b3 <- barplot(pca$rotation[,3], las=2, col="darkblue", main = "3rd component (Dim-3)")
```

In the third dimension the main contributions are from TRBpct and USGpct. The least dimension is TwoPA, FTA, PPG and APG.

```{r,echo=FALSE}
b4 <- barplot(pca$rotation[,4], las=2, col="darkblue", main = "4th component (Dim-4)")
```

In the fourth dimension the main contributions are from DRTG, ASTpct and GP. Tspct, eFGpct are the least contribution.

```{r, include = FALSE, echo=FALSE}
calculateScore = function(data) {
  return(sum((pca$rotation[, 1]*data)^2))
}
player.df$NAME[sort.int(apply(nba, 1, calculateScore), decreasing = T, 
                                           index.return = T)$ix[1:10]]
```

```{r, include = FALSE, echo=FALSE}
calculateScore = function(data) {
  return(sum((pca$rotation[, 2]*data)^2))
}
nba_data$GP[sort.int(apply(nba, 1, calculateScore), decreasing = T, index.return = T)$ix[1:10]]
```

```{r}
fviz_pca_var(pca, col.var = "cos2",
             gradient.cols = brewer.pal(3, "Set1"),
             repel = TRUE,
             title = "Cos 2 of the variables NBA Stats - PCA")
```

In the factorial map above, we observe the relationships between statistical variables of NBA players. We can see that the variables SPG  GP  Ftpct   VI  pct2P  Tspct  FGpcl  are Negatively correlated with the first dimension, while the variables DGRT USGpct  TOPG  APG  MPG  FTA  ThreePA  BPG and TwoBa and positively correlated with the second dimension. Both the dimensions represented on the graph shows 33.4% and 11.5% of the variance of the data respectively. Variables which are positively correlated with high cos2  indicate  a good representation of these axes. Variables which are negatively correlated with low cos2 indicate that the principal axes do not perfectly represent represent the variable. 

```{r,echo=FALSE}
fviz_pca_biplot(pca, col.var = "contrib", labelsize = 3, repel = TRUE, geom = "point", point.size = 2, point.color = "blue", title = "PCA Biplot Individuals and variables")
```

Biplot is an overlay of a score plot. This is a common plot in Principal Component Analysis. A biplot allows information on both samples and variables of a data matrix to be displayed graphically. We see the observations and the variables in the same plane using the first two components. We can see each player listed on this plot, and their names are listed on the plot below.

```{r, include=FALSE, echo=FALSE}
fviz_pca_biplot(pca, col.var = "contrib", labelsize = 3, repel = TRUE, title = "PCA Biplot Individuals and variables", xlab = "PC1", ylab = "PC2")
```

```{r}
data.frame(z1=-pca$x[,1],z2=pca$x[,2]) %>% 
  ggplot(aes(z1,z2,label=names, color=nba$min)) + geom_point(size=0) +
  labs(title="Player ranking by minutes played on the PCA", x="Dim1 (33.4%)", 
       y="Dim2 (11.5%)") +
  theme_bw() + scale_color_gradient(low="red", high="darkblue")+
  theme(legend.position="bottom") + geom_text(size=2, hjust=0.6, vjust=0,
                                              check_overlap = TRUE)+
  guides(color = guide_legend(title = "Minutes played"))
```

This graph shows the distribution of players when looking at the first two principal components

```{r, include=FALSE, echo=FALSE}
#corrplot(correlation_matrix, method = "color", type = "upper", tl.cex = 0.7)
#dendrogram that illustrates hierarchical clustering of variables based on their correlations. This can help identify groups of variables that are more strongly related to each other.
result <- pvclust(correlation_matrix)
```


## Conclusion

This project shows an application of principal component analysis on an NBA data set. The model shows the relevance of each variable on the overall performance of players. The cosine values of each unit vector shows that many stats are closely related to other stats. For example, 2-point attempts and free-throw attempts are closely related. This may be because players shooting close to the rim are more likely to be fouled and receive a free-throw attempt. The four principal components used include combinations of the variables, and the amount that each variable contributes is shown above in the graph. These four components are constructed by creating linear combinations of the variables as unit vectors. This project shows how a data set with 28 variables can be reduced to a model that consists of four principal components. While principal component analysis may make a model that explains slightly less variation, it has simplicity that can be helpful for understanding a complex data set. 

# References
